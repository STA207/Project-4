---
title: "STA 207 Project 4"
author: "Nassim Ali-Chaouche"
date: "2/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
library(MASS)
library(car)
library(randomForest)
library(ggplot2)
library(dplyr)
library(caret)
library(pscl)
library(ResourceSelection)
library(survey)
library(pROC)
library(ROCR)
library(ROSE)
library(groupdata2)
library(tidyr)
library(broom)
library(boot)
```


```{r}
bank.additional.full <- read.csv("~/Desktop/STA 207/Project 4/bank-additional/bank-additional-full.csv", sep=";")
data = bank.additional.full
```

```{r}
# Checking the type of each variable
sapply(bank.additional.full, class)
```


```{r}
#Recoding No as 0 and Yes as 1
data['y'] = ifelse(data$y == 'no','0','1')
data$y = as.factor(data$y)
```

```{r}
# Removing some predictors from the dataset

data = data[,-which(names(data) %in%  c("duration", "loan"))]
```

```{r}
dim(data)
```


```{r}
# Training/Testing Data Sets
set.seed(1)
selection = sample(1:nrow(data), size = ceiling(nrow(data)*0.7), replace = FALSE)
training = data[selection,]
testing = data[-selection,]
```


```{r}
# Accuracy with logistic regression (original data set, no oversampling)
logreg_datatrain<-glm(y ~ ., data = training, family = binomial)
logreg_datatrain_testprob <- predict(logreg_datatrain, testing, type = "response")
logreg_datatrain_testpred <- ifelse(logreg_datatrain_testprob > 0.5, "1", "0")
logreg_data_cm <- table(logreg_datatrain_testpred, testing$y , dnn = c("Predicted","True"))
sum(diag(logreg_data_cm))/sum(logreg_data_cm)
```

```{r}
# Proportion of observations with y = "no" and "yes" in complete, training, and testing data sets
prop.table(table(data$y))
prop.table(table(training$y))
prop.table(table(testing$y))
```


# Creating an upsampled training set with ROSE technique:

```{r}
#Number of observations in each level of y in original training set
table(training$y)
```

```{r}
n_0 <- 25570
new_frac_0 <- 0.50
new_n_total <- n_0/new_frac_0
oversampling_result <- ovun.sample(y ~ ., data = training, method = "over", N = new_n_total, seed = 2020)


oversampled_training <- oversampling_result$data
```

```{r}
#Number of observations in each level of y in oversampled training set
table(oversampled_training$y)
```


# Another Method to Create An Upsampled Training Data Set


```{r}
training_upsample = upsample(training, cat_col = 'y')
```

```{r}
table(training_upsample$y)
```




# Logistic Regression on Upsampled Training Data Set

```{r}
logreg_datatrain2<-glm(y ~ ., data = oversampled_training, family = binomial)
logreg_datatrain_testprob2 <- predict(logreg_datatrain2, testing, type = "response")
logreg_datatrain_testpred2 <- ifelse(logreg_datatrain_testprob2 > 0.5, "1", "0")
logreg_data_cm2 <- table(logreg_datatrain_testpred2, testing$y , dnn = c("Predicted","True"))
sum(diag(logreg_data_cm2))/sum(logreg_data_cm2)
```








### Analysis Plan for Model Evaluation

We will rank the importance of predictors in the logistic regression based on the absolute value of the t-statistics corresponding to each predictor. A Pseudo R^2 measure will be used to test the goodness-of-fit of the model. The recall of the model will be assessed, and the area under of the ROC Curve will be used to examine the predictive ability of the model. 

### Analysis Plan for Model Diagnostics

A scatterplot between each continuous predictor variable and the log odds of the response variable will be used to examine the assumption of the logistic regression model that the predictors are linearly related to the log odds of the response variable. VIF values will be used to examine the assumption that there is low multicollinearity among the predictors. Influential observations will also be discussed.






### Final Logistic Regression Model Evaluation


```{r, include = FALSE}
# Model:
model_std <- glm(y ~ ., family = binomial(link = "logit"),  data = training_upsample)

```

# Important Variables

```{r, include = FALSE}
model_std2 <- train(y ~ ., data = training_upsample, method = "glm", family = binomial(link = "logit"))
```

```{r, include = FALSE}
varImp(model_std2)
```

Some of the most important variables in the regression are "emp.var.rate", "contact:telephone", and "cons.price.idx," based on the absolute value of the t-statistic corresponding to each predictor. 



# Pseudo R^2:

```{r, include = FALSE}
pR2(model_std)

# (Source: http://cowles.yale.edu/sites/default/files/files/pub/d04/d0474.pdf, page 35 for interpretating of the McFadden score)
```

The McFadden Score, which is a measure of goodness-of-fit/reduction in deviance, is 0.23. According to McFadden (1977), values of 0.2 to 0.4 represent an excellent fit. 


# Recall

```{r, include = FALSE}
logreg_datatrain_testprob3 <- predict(model_std, testing, type = "response")
logreg_datatrain_testpred3 <- ifelse(logreg_datatrain_testprob3 > 0.5, "1", "0")
logreg_data_cm3 <- table(logreg_datatrain_testpred3, testing$y , dnn = c("Predicted","True"))
logreg_data_cm3 
```

```{r, include = FALSE}
# Recall/ True Positive Rate

838/(838+540)
```

The recall, or true positive rate, of the logisitic model is 0.61. 61% of the observations in the "Yes" category were correctly classified.



# ROC Curve


```{r, include = FALSE}
prob <- predict(model_std, newdata=testing, type = "response")
pred <- prediction(prob, testing$y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
```

```{r, include = FALSE}
# AUC
# Reference: https://www.r-bloggers.com/evaluating-logistic-regression-models/

auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

The AUC value is approximately 0.79. Values above 0.80 indicate that a model does a good job in differentiating between the "no" and "yes" categories of the response variable. The AUC value of 0.79 is very close to 0.80, and thus our model has good predictive ability according to the AUC measure. 




### Final Logistic Model Diagnostics

```{r, include = FALSE}
probabilities <- predict(model_std, type = "response")
logit = log(probabilities)/(1-probabilities)

```

```{r, include = FALSE}
plot(logit, training_upsample$age)
plot(logit, training_upsample$campaign)
plot(logit, training_upsample$cons.conf.idx)
plot(logit, training_upsample$cons.price.idx)
plot(logit, training_upsample$emp.var.rate)
plot(logit, training_upsample$euribor3m)
plot(logit, training_upsample$nr.employed)
plot(logit, training_upsample$pdays)
plot(logit, training_upsample$previous)
```


```{r, echo = FALSE, fig.align="center"}
plot(model_std, which=c(4))
```

Figure # Visual Diagnostics of Logistic Regression Assumptions. (a) Cook's Distance Plot, showing the Cook's distance value for every observation.


# Linearity between the continuous predictor variables and the log odds of the response variable

After creating scatterplots between the log odds of the response variable and the continuous predictor variables (9 in total), each of the predictor variables displayed a non-linear relationship with the log odds of the response variable. Under time constraints we did not have the oppurtunity to try transformations of the variables.

# Multicollinearity

```{r, include = FALSE}
vif(model_std)
```

Most variables have VIF values of less than 10. A few variables (emp.var.rate, euribor3m, nr.employed) have VIF values very slightly above 10. Overall, multicollinearity does not pose a big issue in the analysis.


# Influential observations

In Figure #(a), the Cook's distance for every observation is extremely small, with the largest Cook's distance value being less than 0.003. Thus, there are no highly influential observations.
















# Random Forest with Oversampling on Training Data Set 



```{r}
model3 <- randomForest(y ~ ., data = oversampled_training, importance = TRUE)
```

```{r}
model3
```


```{r}
# Tuning to find optimal mtry value (default is the square root of the number of predictors)
# ncol(oversampled_training) refers to the last column of the oversampled_training data set (last column is the response variable)

bestmtry2 <- tuneRF(oversampled_training[,-ncol(oversampled_training)], oversampled_training[,ncol(oversampled_training)], ntreeTry = 500, improve = 1e-5, stepFactor = 1.5, doBest = TRUE)
```

```{r}
print(bestmtry2)
```

```{r}
model4 <- randomForest(y ~ ., data = oversampled_training, mtry = 13, importance = TRUE)
```

```{r}
print(model4)
```


```{r}
# Predicting on the Testing Set
predtesting2 <- predict(model4, testing, type = "class")

# Accuracy 
mean(predtesting2 == testing$y)

# Confusion Matrix
table(predtesting2, testing$y)

```

```{r}
# True Positive

528/(850+528)


```


```{r}
# Checking for important variables
importance(model4)
```

```{r}
varImpPlot(model4)
```

```{r}
# Bar Graph with Importance ranked by MeanDecreaseAccuracy
# Reference: https://towardsdatascience.com/implement-random-forest-in-r-b00b69eb8501
importance = importance(model4)
varImportance = data.frame(Variables = row.names(importance),
 Importance =round(importance[, "MeanDecreaseAccuracy"],2))
rankImportance=varImportance%>%mutate(Rank=paste('#',dense_rank(desc(Importance))))
ggplot(rankImportance,aes(x=reorder(Variables,Importance),
 y=Importance,fill=Importance))+ 
 geom_bar(stat='identity') + 
 geom_text(aes(x = Variables, y = 0.5, label = Rank),
 hjust=0, vjust=0.55, size = 4, colour = 'white') +
 labs(x = 'Variables') +
 coord_flip() + 
 theme_classic()
```

```{r}
# Bar Graph with Importance ranked by MeanDecreaseGini
importance = importance(model4)
varImportance = data.frame(Variables = row.names(importance),
 Importance =round(importance[, "MeanDecreaseGini"],2))
rankImportance=varImportance%>%mutate(Rank=paste('#',dense_rank(desc(Importance))))
ggplot(rankImportance,aes(x=reorder(Variables,Importance),
 y=Importance,fill=Importance))+ 
 geom_bar(stat='identity') + 
 geom_text(aes(x = Variables, y = 0.5, label = Rank),
 hjust=0, vjust=0.55, size = 4, colour = 'white') +
 labs(x = 'Variables') +
 coord_flip() + 
 theme_classic()
```











