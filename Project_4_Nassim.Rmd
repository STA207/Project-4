---
title: "STA 207 Project 4"
author: "Nassim Ali-Chaouche"
date: "2/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
library(MASS)
library(car)
library(randomForest)
library(ggplot2)
library(dplyr)
library(caret)
library(pscl)
library(ResourceSelection)
library(survey)
library(pROC)
library(ROCR)
library(ROSE)
library(groupdata2)
library(tidyr)
library(broom)
library(boot)
```


```{r}
bank.additional.full <- read.csv("~/Desktop/STA 207/Project 4/bank-additional/bank-additional-full.csv", sep=";")
data = bank.additional.full
```

```{r}
# Checking the type of each variable
sapply(bank.additional.full, class)
```


```{r}
#Recoding No as 0 and Yes as 1
data['y'] = ifelse(data$y == 'no','0','1')
data$y = as.factor(data$y)
```

```{r}
# Removing some predictors from the dataset

data = data[,-which(names(data) %in%  c("duration", "loan"))]
```

```{r}
dim(data)
```


```{r}
# Training/Testing Data Sets
set.seed(1)
selection = sample(1:nrow(data), size = ceiling(nrow(data)*0.7), replace = FALSE)
training = data[selection,]
testing = data[-selection,]
```


```{r}
# Accuracy with logistic regression (original data set, no oversampling)
logreg_datatrain<-glm(y ~ ., data = training, family = binomial)
logreg_datatrain_testprob <- predict(logreg_datatrain, testing, type = "response")
logreg_datatrain_testpred <- ifelse(logreg_datatrain_testprob > 0.5, "1", "0")
logreg_data_cm <- table(logreg_datatrain_testpred, testing$y , dnn = c("Predicted","True"))
sum(diag(logreg_data_cm))/sum(logreg_data_cm)
```

```{r}
# Proportion of observations with y = "no" and "yes" in complete, training, and testing data sets
prop.table(table(data$y))
prop.table(table(training$y))
prop.table(table(testing$y))
```


# Creating an upsampled training set with ROSE technique:

```{r}
#Number of observations in each level of y in original training set
table(training$y)
```

```{r}
n_0 <- 25570
new_frac_0 <- 0.50
new_n_total <- n_0/new_frac_0
oversampling_result <- ovun.sample(y ~ ., data = training, method = "over", N = new_n_total, seed = 2020)


oversampled_training <- oversampling_result$data
```

```{r}
#Number of observations in each level of y in oversampled training set
table(oversampled_training$y)
```


# Another Method to Create An Upsampled Training Data Set


```{r}
training_upsample = upsample(training, cat_col = 'y')
```

```{r}
table(training_upsample$y)
```




# Logistic Regression on Upsampled Training Data Set

```{r}
logreg_datatrain2<-glm(y ~ ., data = oversampled_training, family = binomial)
logreg_datatrain_testprob2 <- predict(logreg_datatrain2, testing, type = "response")
logreg_datatrain_testpred2 <- ifelse(logreg_datatrain_testprob2 > 0.5, "1", "0")
logreg_data_cm2 <- table(logreg_datatrain_testpred2, testing$y , dnn = c("Predicted","True"))
sum(diag(logreg_data_cm2))/sum(logreg_data_cm2)
```








### Analysis Plan for Model Evaluation

We will rank the importance of predictors in the logistic regression based on the absolute value of the t-statistics corresponding to each predictor. A Pseudo R^2 measure will be used to test the goodness-of-fit of the model. The recall of the model will be assessed, and the area under of the ROC Curve will be used to examine the predictive ability of the model. 

### Analysis Plan for Model Diagnostics

A scatterplot between each continuous predictor variable and the log odds of the response variable will be used to examine the assumption of the logistic regression model that the predictors are linearly related to the log odds of the response variable. VIF values will be used to examine the assumption that there is low multicollinearity among the predictors. Influential observations will also be discussed.






### Final Logistic Regression Model Evaluation


```{r, include = FALSE}
# Model:
model_std <- glm(y ~ ., family = binomial(link = "logit"),  data = training_upsample)

```

# Important Variables

```{r, include = FALSE}
model_std2 <- train(y ~ ., data = training_upsample, method = "glm", family = binomial(link = "logit"))
```

```{r, include = FALSE}
varImp(model_std2)
```

Some of the most important variables in the regression are "emp.var.rate", "contact:telephone", and "cons.price.idx," based on the absolute value of the t-statistic corresponding to each predictor. 



# Pseudo R^2:

```{r, include = FALSE}
pR2(model_std)

# (Source: http://cowles.yale.edu/sites/default/files/files/pub/d04/d0474.pdf, page 35 for interpretating of the McFadden score)
```

The McFadden Score, which is a measure of goodness-of-fit/reduction in deviance, is 0.23. According to McFadden (1977), values of 0.2 to 0.4 represent an excellent fit. 


# Recall

```{r, include = FALSE}
logreg_datatrain_testprob3 <- predict(model_std, testing, type = "response")
logreg_datatrain_testpred3 <- ifelse(logreg_datatrain_testprob3 > 0.5, "1", "0")
logreg_data_cm3 <- table(logreg_datatrain_testpred3, testing$y , dnn = c("Predicted","True"))
logreg_data_cm3 
```

```{r, include = FALSE}
# Recall/ True Positive Rate

838/(838+540)
```

The recall, or true positive rate, of the logisitic model is 0.61. 61% of the observations in the "Yes" category were correctly classified.



# ROC Curve


```{r, include = FALSE}
prob <- predict(model_std, newdata=testing, type = "response")
pred <- prediction(prob, testing$y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
```

```{r, include = FALSE}
# AUC
# Reference: https://www.r-bloggers.com/evaluating-logistic-regression-models/

auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

The AUC value is approximately 0.79. Values above 0.80 indicate that a model does a good job in differentiating between the "no" and "yes" categories of the response variable. The AUC value of 0.79 is very close to 0.80, and thus our model has good predictive ability according to the AUC measure. 




### Final Logistic Model Diagnostics

```{r, include = FALSE}
probabilities <- predict(model_std, type = "response")
logit = log(probabilities)/(1-probabilities)

```

```{r, include = FALSE}
plot(logit, training_upsample$age)
plot(logit, training_upsample$campaign)
plot(logit, training_upsample$cons.conf.idx)
plot(logit, training_upsample$cons.price.idx)
plot(logit, training_upsample$emp.var.rate)
plot(logit, training_upsample$euribor3m)
plot(logit, training_upsample$nr.employed)
plot(logit, training_upsample$pdays)
plot(logit, training_upsample$previous)
```


```{r, echo = FALSE, fig.align="center"}
plot(model_std, which=c(4))
```

Figure # Visual Diagnostics of Logistic Regression Assumptions. (a) Cook's Distance Plot, showing the Cook's distance value for every observation.


# Linearity between the continuous predictor variables and the log odds of the response variable

After creating scatterplots between the log odds of the response variable and the continuous predictor variables (9 in total), each of the predictor variables displayed a non-linear relationship with the log odds of the response variable. Under time constraints we did not have the oppurtunity to try transformations of the variables.

# Multicollinearity

```{r, include = FALSE}
vif(model_std)
```

Most variables have VIF values of less than 10. A few variables (emp.var.rate, euribor3m, nr.employed) have VIF values very slightly above 10. Overall, multicollinearity does not pose a big issue in the analysis.


# Influential observations

In Figure #(a), the Cook's distance for every observation is extremely small, with the largest Cook's distance value being less than 0.003. Thus, there are no highly influential observations.






### Explaining the Difference in Naive Bayes, Logistic Regression, and Random Forest

The Naive Bayes is a classification algorithm based on Bayes rule and a set of conditional independence assumptions. The algorithm makes the assumption that each Xi is conditionally independent of the other predictors given the response variable, as well independent of each subset of the other predictors given the response variable. Logistic Regression assumes a parametric form for the distribution P(Y|X), and then directly estimates the parameters from the training data. The parametric form of P(Y|X) used by Logistic Regression is the form implied by the Guassian Naive Bayes approach, and thus logistic regression is a close alternative to the Guassian Naive Bayes approach. When the assumptions of the Guassian Naive Bayes model do not hold, Logistic Regression and Guassian Naive Bayes typically learn different classifier functions (Mitchell 2015). Thus, the slight gap between the performances of the Logistic Regression and Naive Bayes approaches for our project can be due to the data not following all the assumptions of the Guassian Naive Bayes approach. For instance, the recall measure of the optimal Naive Bayes model of 0.70 is slighter higher than the recall measure of Logistic Regression model of 0.61. According to Ng and Jordan (2002), Logistic Regression and Guassian Naive Bayes approach their asymptotic accuracies at different rates (asypmtotic referring to as the sample size goes to infinity). They state that "as the number of training examples m is increased, one would expect generative naive Bayes to initially do better, but for discriminant logistic regression to eventually catch up to, and quite likely overtake, the performance of Naive Bayes." Thus, it is possible that if we had a larger sample size for the training set, that the performance with Logistic Regression would be better than that of the Naive Bayes approach. In terms of Random Forest, it is an ensemble-based learning algorithm which is comprised of n collections of de-correlated decision trees, and uses multiple trees to average or compute majority votes in the terminal leaf nodes when making a prediction (Kirasich et. al, 2018). Thus, it is evidently a completely different approach compared to Logistic Regression and Naive Bayes, whose performance compared to the other approaches will vary depending on the nature of the data set. Couronne et. al 2018 state that "the superiority of Random Forest tends to be more pronounced for increasing p and p/n". Since the number of predictors in our data set is relatively small (less than 20), this is most likely the reason why the Random Forest does not perform as well as Logistic Regression and Naive Bayes. (For instance, the recall of our Random Forest model is 0.57 which is less than the Logistic Regression and Naive Bayes approaches.)    





```{r}
#Sources:
#https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf
#http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf
#https://scholar.smu.edu/cgi/viewcontent.cgi?article=1041&context=datasciencereview
#https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2264-5
```










# Random Forest with Oversampling on Training Data Set 



```{r}
model3 <- randomForest(y ~ ., data = oversampled_training, importance = TRUE)
```

```{r}
model3
```


```{r}
# Tuning to find optimal mtry value (default is the square root of the number of predictors)
# ncol(oversampled_training) refers to the last column of the oversampled_training data set (last column is the response variable)

bestmtry2 <- tuneRF(oversampled_training[,-ncol(oversampled_training)], oversampled_training[,ncol(oversampled_training)], ntreeTry = 500, improve = 1e-5, stepFactor = 1.5, doBest = TRUE)
```

```{r}
print(bestmtry2)
```

```{r}
model4 <- randomForest(y ~ ., data = oversampled_training, mtry = 13, importance = TRUE)
```

```{r}
print(model4)
```


```{r}
# Predicting on the Testing Set
predtesting2 <- predict(model4, testing, type = "class")

# Accuracy 
mean(predtesting2 == testing$y)

# Confusion Matrix
table(predtesting2, testing$y)

```

```{r}
# True Positive

528/(850+528)


```


```{r}
# Checking for important variables
importance(model4)
```

```{r}
varImpPlot(model4)
```

```{r}
# Bar Graph with Importance ranked by MeanDecreaseAccuracy
# Reference: https://towardsdatascience.com/implement-random-forest-in-r-b00b69eb8501
importance = importance(model4)
varImportance = data.frame(Variables = row.names(importance),
 Importance =round(importance[, "MeanDecreaseAccuracy"],2))
rankImportance=varImportance%>%mutate(Rank=paste('#',dense_rank(desc(Importance))))
ggplot(rankImportance,aes(x=reorder(Variables,Importance),
 y=Importance,fill=Importance))+ 
 geom_bar(stat='identity') + 
 geom_text(aes(x = Variables, y = 0.5, label = Rank),
 hjust=0, vjust=0.55, size = 4, colour = 'white') +
 labs(x = 'Variables') +
 coord_flip() + 
 theme_classic()
```

```{r}
# Bar Graph with Importance ranked by MeanDecreaseGini
importance = importance(model4)
varImportance = data.frame(Variables = row.names(importance),
 Importance =round(importance[, "MeanDecreaseGini"],2))
rankImportance=varImportance%>%mutate(Rank=paste('#',dense_rank(desc(Importance))))
ggplot(rankImportance,aes(x=reorder(Variables,Importance),
 y=Importance,fill=Importance))+ 
 geom_bar(stat='identity') + 
 geom_text(aes(x = Variables, y = 0.5, label = Rank),
 hjust=0, vjust=0.55, size = 4, colour = 'white') +
 labs(x = 'Variables') +
 coord_flip() + 
 theme_classic()
```











