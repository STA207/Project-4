knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(car)
library(randomForest)
library(ggplot2)
library(dplyr)
library(caret)
install.packages("caret")
install.packages("pscl")
install.packages("MKmisc")
install.packages("ResourceSelection")
install.packages("survey")
install.packages("pROC")
install.packages("ROCR")
library(MASS)
library(car)
library(randomForest)
library(ggplot2)
library(dplyr)
library(caret)
library(pscl)
library(MKmisc)
install.packages("limma")
library(MASS)
library(car)
library(randomForest)
library(ggplot2)
library(dplyr)
library(caret)
library(pscl)
library(MKmisc)
library(MASS)
library(car)
library(randomForest)
library(ggplot2)
library(dplyr)
library(caret)
library(pscl)
library(ResourceSelection)
library(survey)
library(pROC)
library(ROCR)
knitr::opts_chunk$set(echo = TRUE)
bank.additional.full <- read.csv("~/Desktop/STA 207/Project 4/bank-additional/bank-additional-full.csv", sep=";")
data = bank.additional.full
library(MASS)
library(car)
library(randomForest)
library(ggplot2)
library(dplyr)
library(caret)
library(pscl)
library(ResourceSelection)
library(survey)
library(pROC)
library(ROCR)
library(ROSE)
library(groupdata2)
#Recoding No as 0 and Yes as 1
data['y'] = ifelse(data$y == 'no','0','1')
data$y = as.factor(data$y)
# Removing some predictors from the dataset
data = data[,-which(names(data) %in%  c("duration", "loan"))]
dim(data)
# Proportion of observations with y = "no" and "yes" in complete, training, and testing data sets
prop.table(table(data$y))
prop.table(table(training$y))
# Training/Testing Data Sets
set.seed(1)
selection = sample(1:nrow(data), size = ceiling(nrow(data)*0.7), replace = FALSE)
training = data[selection,]
testing = data[-selection,]
# Accuracy with logistic regression (original data set, no oversampling)
logreg_datatrain<-glm(y ~ ., data = training, family = binomial)
logreg_datatrain_testprob <- predict(logreg_datatrain, testing, type = "response")
logreg_datatrain_testpred <- ifelse(logreg_datatrain_testprob > 0.5, "1", "0")
logreg_data_cm <- table(logreg_datatrain_testpred, testing$y , dnn = c("Predicted","True"))
sum(diag(logreg_data_cm))/sum(logreg_data_cm)
# Proportion of observations with y = "no" and "yes" in complete, training, and testing data sets
prop.table(table(data$y))
prop.table(table(training$y))
prop.table(table(testing$y))
logreg_datatrain2<-glm(y ~ ., data = oversampled_training, family = binomial)
n_0 <- 25570
new_frac_0 <- 0.50
new_n_total <- n_0/new_frac_0
oversampling_result <- ovun.sample(y ~ ., data = training, method = "over", N = new_n_total, seed = 2020)
oversampled_training <- oversampling_result$data
#Number of observations in each level of y in oversampled training set
table(oversampled_training$y)
logreg_datatrain2<-glm(y ~ ., data = oversampled_training, family = binomial)
logreg_datatrain_testprob2 <- predict(logreg_datatrain2, testing, type = "response")
logreg_datatrain_testpred2 <- ifelse(logreg_datatrain_testprob2 > 0.5, "1", "0")
logreg_data_cm2 <- table(logreg_datatrain_testpred2, testing$y , dnn = c("Predicted","True"))
sum(diag(logreg_data_cm2))/sum(logreg_data_cm2)
ncol(training)
model3 <- randomForest(y ~ ., data = oversampled_training, importance = TRUE)
model3
# Tuning to find optimal mtry value (default is the square root of the number of predictors)
# ncol(training) refers to the last column of the training data set (last column is the response variable)
bestmtry2 <- tuneRF(oversampled_training[,-ncol(oversampled_training)], oversampled_training[,ncol(oversampled_training)], ntreeTry = 500, improve = 1e-5, stepFactor = 1.5, doBest = TRUE)
print(bestmtry2)
